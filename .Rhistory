df1$CACV=Q0$CAC0*(df1$CACD/lagum(df1$CACD))
df1$NKV=Q0$NK0*(df1$NKD/lagum(df1$NKD))
df1=subset(df1,!is.na(DJV))
# portfolio value
df1$QVAL=df1$DJV+df1$FTV+df1$CACV+df1$NKV
# potential one day losses
df1$QLoss=QVal0-df1$QVAL
##########################################################################
# eliminate pull scenario data to match text book
tmp=subset(df1,!is.na(QVAL))
tmp=orderBy(~day,data=tmp)
tmp$scenario=1:nrow(tmp)
df3=tmp[,c('scenario','date','DJS','FTS','CACS','NKS',
'DJV','FTV','CACV','NKV','QVAL','QLoss')]
View(df3)
View(tmp)
##########################################################################
# sort losses in descending order\\
df4=orderBy(~-QLoss,data=df3)
# compute empirical pvalues for Qlosses
df4$pvalQ=apply(as.matrix(df4$QLoss),1,pvalx,x=df4$QLoss)
#Find our Var and cVar
(VaR99_1Day=df4$QLoss[5])
(cVaR99_1Day=round(mean(df4$QLoss[1:4]),2))
#######################
#b
lambda=0.995
df5=df4[,c('scenario','QLoss')]
View(df5)
#######################
#b
lambda=0.995
df5=df4[,c('scenario','QLoss')]
(n=nrow(df5))
df5$wts=lambda^(n-df5$scenario)*(1-lambda)/(1-lambda^n)
df5$cumwts=cumsum(df5$wts)
(pickum=min(which(df5$cumwts>=0.01)))
df5[pickum,]
(pickum=min(which(df5$cumwts>=0.01)))
# compute VaR99_1day_wts and cVaR99_1Day_wts
(df6=df5[1:(pickum-1),])
View(df6)
df6$cprob=df6$wts/sum(df6$wts)
# compute VaR99_1day_wts and cVaR99_1Day_wts
(df6=df5[1:(pickum-1),])
df6$cprob=df6$wts/sum(df6$wts)
#Weighted Var and cVar values
(VaR99_1Day_wts=df5$QLoss[pickum])
(cVaR99_1Day_wts=sum(df6$QLoss*df6$cprob))
#c
set.seed(1001)
nreps=2000
#c
set.seed(1001)
nreps=10000
VaRboot=0
cVaRboot=0
j=1
for(j in 1:nreps){
x=sort(sample(df3$QLoss,500,replace=T),decreasing = T)
(VaRboot[j]=x[5])
(cVaRboot[j]=mean(x[1:4]))
if(j==1|j%%250==0) print(paste('rep',j,'of',nreps))
}
# construct bias corrected VaR and cVaR estimates
VaREST = VaR99_1Day - (VaRboot-VaR99_1Day)
cVaREST = cVaR99_1Day - (cVaRboot - cVaR99_1Day)
quantile(VaREST,probs=c(0.025,0.975),type=6)
quantile(cVaREST,probs=c(0.025,0.975),type=6)
#d
VaRboot_wts=0
cVaRboot_wts=0
j=1
for(j in 1:nreps){
x=sort(sample(df5$QLoss,500,replace=T,prob = df5$wts),decreasing = T)
(VaRboot_wts[j]=x[5])
(cVaRboot_wts[j]=mean(x[1:4]))
if(j==1|j%%250==0) print(paste('rep',j,'of',nreps))
}
VaREST_wts = VaR99_1Day_wts - (VaRboot_wts-VaR99_1Day_wts)
cVaREST_wts = cVaR99_1Day_wts - (cVaRboot_wts - cVaR99_1Day_wts)
quantile(VaREST_wts,probs=c(0.025,0.975),type=6)
quantile(cVaREST_wts,probs=c(0.025,0.975),type=6)
##########################################################################
VaR99_1Day_wts
quantile(VaREST_wts,probs=c(0.025,0.975),type=6)
##########################################################################
cVaR99_1Day_wts
quantile(cVaREST_wts,probs=c(0.025,0.975),type=6)
#2
(Q0=data.frame(cbind(DJ0=4000,FT0=3000,CAC0=1000,NK0=2000)))
Q0
(QVal0=sum(Q0[1,]))
##########################################################################
# Load data in HULL's spreadsheet
df0=read.csv('C:/Users/maxel/Desktop/EFIN 401//HULL-CH13-SimulationData.csv')
# Perform our own conversion to US dollars
df0$DJ=df0$DJIA
df0$FTD=df0$FTSE100*df0$FX_USD_GBP
df0$CACD=df0$CAC40/df0$FX_EUR_USD
df0$NKD=df0$Nikkei/df0$FX_YEN_USD
df1=subset(df0,select=c(day,date,DJ,FTD,CACD,NKD))
##########################################################################
# pull data from last day
(n=nrow(df1))
(df2=df1[nrow(df1),])
##########################################################################
# scenarios for possible Sept 26 levels Excel sheet "Scenarios"
df1$DJS=df2$DJ*(df1$DJ/lagum(df1$DJ))
df1$FTS=df2$FTD*(df1$FTD/lagum(df1$FTD))
df1$CACS=df2$CACD*(df1$CACD/lagum(df1$CACD))
df1$NKS=df2$NKD*(df1$NKD/lagum(df1$NKD))
df1$DJV=Q0$DJ0*(df1$DJ/lagum(df1$DJ))
df1$FTV=Q0$FT0*(df1$FTD/lagum(df1$FTD))
df1$CACV=Q0$CAC0*(df1$CACD/lagum(df1$CACD))
df1$NKV=Q0$NK0*(df1$NKD/lagum(df1$NKD))
df1=subset(df1,!is.na(DJV))
# portfolio value
df1$QVAL=df1$DJV+df1$FTV+df1$CACV+df1$NKV
# potential one day losses
df1$QLoss=QVal0-df1$QVAL
# eliminate pull scenario data to match text book
tmp=subset(df1,!is.na(QVAL))
tmp=orderBy(~day,data=tmp)
tmp$scenario=1:nrow(tmp)
df3=tmp[,c('scenario','date','DJS','FTS','CACS','NKS',
'DJV','FTV','CACV','NKV','QVAL','QLoss')]
# sort losses in descending order  - spreadsheet tab 3
df4=orderBy(~-QLoss,data=df3)
# compute empirical pvalues for Qlosses
df4$pvalQ=apply(as.matrix(df4$QLoss),1,pvalx,x=df4$QLoss)
head(df4[,c('scenario','QLoss','pvalQ')],15)
#Var and cVar values
(VaR99_1Day=df4$QLoss[5])
#Var and cVar values
(VaR99_1Day=df4$QLoss[5])
(cVaR99_1Day=round(mean(df4$QLoss[1:4]),2))
lambda=0.99
df5=df4[,c('scenario','QLoss')]
(n=nrow(df5))
df5$wts=lambda^(n-df5$scenario)*(1-lambda)/(1-lambda^n)
df5$cumwts=cumsum(df5$wts)
# see HULL Table 13.5 page 302  or spreadsheet tab 5
(pickum=min(which(df5$cumwts>=0.01)))
df5[pickum,]
# compute VaR99_1day_wts and cVaR99_1Day_wts
(df6=df5[1:(pickum-1),])
df6$cprob=df6$wts/sum(df6$wts)
#New weighted Var and cVar values
(VaR99_1Day_wts=df5$QLoss[pickum])
(cVaR99_1Day_wts=sum(df6$QLoss*df6$cprob))
#Var and cVar values
(VaR99_1Day=df4$QLoss[5])
(cVaR99_1Day=round(mean(df4$QLoss[1:4]),2))
#New weighted Var and cVar values
(VaR99_1Day_wts=df5$QLoss[pickum])
(cVaR99_1Day_wts=sum(df6$QLoss*df6$cprob))
add.julian=function(df0){
df0$Date=row.names(df0)
df0$jday=as.numeric(julian(as.Date(df0$Date,"%Y-%m-%d")))
df0
}
getSymbols("DEXUSUK",src="FRED")
getSymbols("DEXUSEU",src="FRED")
getSymbols("DEXJPUS",src="FRED")
tmp=as.data.frame(DEXUSUK)
View(tmp)
View(tmp)
tmp=as.data.frame(DEXUSUK)
tmp$Date=row.names(tmp); tmp=add.julian(tmp);  tmp$USDGBP=tmp$DEXUSUK
row.names(tmp)=NULL; USD_GBP=tmp[,c('jday','USDGBP')]
tmp=as.data.frame(DEXUSEU)
tmp$Date=row.names(tmp); tmp=add.julian(tmp); tmp$USDEUR=tmp$DEXUSEU
row.names(tmp)=NULL; USD_EUR=tmp[,c('jday','USDEUR')]
tmp=as.data.frame(DEXJPUS)
tmp$Date=row.names(tmp); tmp=add.julian(tmp); tmp$USDJPY=1/tmp$DEXJPUS
row.names(tmp)=NULL; USD_JPY=tmp[,c('jday','USDJPY')]
dfFX=merge(USD_GBP,USD_EUR)
View(dfFX)
dfFX=merge(dfFX,USD_JPY)
month.day.year(min(dfFX$jday))
# Stock Index Data
getSymbols("^DJI",src="yahoo",from="1990-01-01")
getSymbols("^FTSE",src="yahoo",from="1990-01-01")
getSymbols("^FCHI",src="yahoo",from="1990-01-01")
getSymbols("^N225",src="yahoo",from="1990-01-01")
DJI=as.data.frame(DJI)
FTSE100=as.data.frame(FTSE)
CAC40=as.data.frame(FCHI)
NIK225=as.data.frame(N225)
# list containing Yahoo data objects to process
list1=list(DJI,FTSE100,CAC40,NIK225)
list2=c('DJI','FTSE100','CAC40','NIK225')
j=1
for(j in 1:length(list1)) {
tmp=list1[[j]]
tmp=add.julian(tmp)
(jpick=which(str_detect(names(tmp), "Adjusted")==T))
(ipick=which(str_detect(names(tmp), "jday")==T))
tmp=tmp[,c(ipick,jpick)]
names(tmp) = c('jday',list2[j])
head(tmp)
if(j==1) dfS=tmp
if(j>1)  dfS=merge(dfS,tmp)
}
View(dfS)
View(DJI)
month.day.year(max(dfS$jday))
FXS=merge(dfS,dfFX)
##########################################################################
# compute 99%  LOSS VaR for each 500 day block of data
##########################################################################
# historical
df1$VaR99 = NA
df1$cVaR99 = NA
df1$VaR99wt = NA
df1$cVaR99wt = NA
i=501
i=501
for(i in 501:nrow(df1)){
df2=df1[(i-500):(i-1),]
df2$LOSS=df2$LOSS-mean(df2$LOSS)
df2$wts=wts
(VaR99=as.numeric(quantile(df2$LOSS,probs=0.99)))
(cVaR99=mean(df2$LOSS[df2$LOSS>VaR99]))
df1$VaR99[i]=VaR99
df1$cVaR99[i]=cVaR99
# weighted
df3=orderBy(~-LOSS,data=df2)
df3$cumprob=cumsum(df3$wts)
(pick=min(which(df3$cumprob>0.01)))
(df1$VaR99wt[i]=df3$LOSS[pick])
df4=df3[1:(pick-1),]
(df1$cVaR99wt[i]=sum(df4$LOSS*df4$wts/sum(df4$wts)))
if(i==501|i%%250==0|i==nrow(df1)) print(paste('day',i,'of',nrow(df1)))
}
# derive weights for weighted process
T=500; day=1:T
lambwt=0.995
wts=lambwt^(T-day)*((1-lambwt)/(1-lambwt^T))
##########################################################################
# compute 99%  LOSS VaR for each 500 day block of data
##########################################################################
# historical
df1$VaR99 = NA
df1$cVaR99 = NA
df1$VaR99wt = NA
df1$cVaR99wt = NA
i=501
for(i in 501:nrow(df1)){
df2=df1[(i-500):(i-1),]
df2$LOSS=df2$LOSS-mean(df2$LOSS)
df2$wts=wts
(VaR99=as.numeric(quantile(df2$LOSS,probs=0.99)))
(cVaR99=mean(df2$LOSS[df2$LOSS>VaR99]))
df1$VaR99[i]=VaR99
df1$cVaR99[i]=cVaR99
# weighted
df3=orderBy(~-LOSS,data=df2)
df3$cumprob=cumsum(df3$wts)
(pick=min(which(df3$cumprob>0.01)))
(df1$VaR99wt[i]=df3$LOSS[pick])
df4=df3[1:(pick-1),]
(df1$cVaR99wt[i]=sum(df4$LOSS*df4$wts/sum(df4$wts)))
if(i==501|i%%250==0|i==nrow(df1)) print(paste('day',i,'of',nrow(df1)))
}
#b
(Q0=data.frame(cbind(DJ0=4000,FT0=3000,CAC0=1000,NK0=2000)))
(QVAL0=sum(Q0[1,]))
load('Data-FX-Stock-Indexes.RData')
#Convert to US dollars
df0=FXS
df0$DJ=df0$DJI
df0$FTD=df0$FTSE100*df0$USDGBP
df0$CACD=df0$CAC40*df0$USDEUR
df0$NKD=df0$NIK225*df0$USDJPY
df1=subset(df0,select=c(jday,DJ,FTD,CACD,NKD))
save(df1,file='DJ_FTD_CACD_NKD_Longterm.RData')
df1$RR_DJ  = df1$DJ/lagum(df1$DJ)
df1$RR_FT  = df1$FTD/lagum(df1$FTD)
df1$RR_CAC = df1$CACD/lagum(df1$CACD)
df1$RR_NK  = df1$NKD/lagum(df1$NKD)
df1$DJV  = Q0$DJ0 * df1$RR_DJ
df1$FTV  = Q0$FT0 * df1$RR_FT
df1$CACV = Q0$CAC0 * df1$RR_CAC
df1$NKV  = Q0$NK0 * df1$RR_NK
df1=subset(df1,!is.na(DJV))
# potential one-day-ahead portfolio values and losses
df1$QVAL=df1$DJV+df1$FTV+df1$CACV+df1$NKV
df1$LOSS=df1$QVAL-QVAL0
# derive weights for weighted process
T=500; day=1:T
lambwt=0.995
wts=lambwt^(T-day)*((1-lambwt)/(1-lambwt^T))
##########################################################################
# compute 99%  LOSS VaR for each 500 day block of data
##########################################################################
# historical
df1$VaR99 = NA
df1$cVaR99 = NA
df1$VaR99wt = NA
df1$cVaR99wt = NA
i=501
for(i in 501:nrow(df1)){
df2=df1[(i-500):(i-1),]
df2$LOSS=df2$LOSS-mean(df2$LOSS)
df2$wts=wts
(VaR99=as.numeric(quantile(df2$LOSS,probs=0.99)))
(cVaR99=mean(df2$LOSS[df2$LOSS>VaR99]))
df1$VaR99[i]=VaR99
df1$cVaR99[i]=cVaR99
# weighted
df3=orderBy(~-LOSS,data=df2)
df3$cumprob=cumsum(df3$wts)
(pick=min(which(df3$cumprob>0.01)))
(df1$VaR99wt[i]=df3$LOSS[pick])
df4=df3[1:(pick-1),]
(df1$cVaR99wt[i]=sum(df4$LOSS*df4$wts/sum(df4$wts)))
if(i==501|i%%250==0|i==nrow(df1)) print(paste('day',i,'of',nrow(df1)))
}
##########################################################################
# compute 99%  LOSS VaR for each 500 day block of data
##########################################################################
# historical
df1$VaR99 = NA
df1$cVaR99 = NA
df1$VaR99wt = NA
df1$cVaR99wt = NA
i=501
for(i in 501:nrow(df1)){
df2=df1[(i-500):(i-1),]
df2$LOSS=df2$LOSS-mean(df2$LOSS)
df2$wts=wts
(VaR99=as.numeric(quantile(df2$LOSS,probs=0.99)))
(cVaR99=mean(df2$LOSS[df2$LOSS>VaR99]))
df1$VaR99[i]=VaR99
df1$cVaR99[i]=cVaR99
# weighted
df3=orderBy(~-LOSS,data=df2)
df3$cumprob=cumsum(df3$wts)
(pick=min(which(df3$cumprob>0.01)))
(df1$VaR99wt[i]=df3$LOSS[pick])
df4=df3[1:(pick-1),]
(df1$cVaR99wt[i]=sum(df4$LOSS*df4$wts/sum(df4$wts)))
if(i==501|i%%250==0|i==nrow(df1)) print(paste('day',i,'of',nrow(df1)))
}
df1$hit99=ifelse(df1$LOSS>df1$VaR99,1,0)
df1$hit99wt=ifelse(df1$LOSS>df1$VaR99wt,1,0)
#c
sum(df1$hit99[501:length(df1$hit99)])/(length(df1$hit99))
sum(df1$hit99wt[501:length(df1$hit99wt)])/length(df1$hit99wt)
View(df2)
install.packages('StMoMo')
#Lee Carter Modeling
library(StMoMo)
install.packages('demography')
library(demography)
#Lee Carter Modeling
library(StMoMo)
library(demography)
read.csv('C:/Users/maxel/Desktop/EFIN 401/USAMortality.csv')
USA <- read.csv('C:/Users/maxel/Desktop/EFIN 401/USAMortality.csv')
rm(list=ls())
USA <- read.csv('C:/Users/maxel/Desktop/EFIN 401/USAMortality.csv')
View(USA)
AUSdata <- hmd.mx(country = 'AUS', username = 'maxelli1201@gmail.com',
password = 'Fisher1930')
AUSdata <- hmd.mx(country = 'AUS', username = 'maxelli1201@gmail.com',
password = 'Fisher1930$')
View(AUSdata)
plot(AUSdata, series = 'male')
#Fit the Lee Carter Model
LCFit <- fit(LC, data = AUSmale, ages.fit = agesFit,
years.fit = yearsFit)
#Lee Carter Modeling
library(StMoMo)
library(demography)
rm(list=ls())
AUSdata <- hmd.mx(country = 'AUS', username = 'maxelli1201@gmail.com',
password = 'Fisher1930$')
plot(AUSdata, series = 'male')
AUSmale <- StMoMo(AUSdata, series = 'male')
AUSmale <- StMoMoData(AUSdata, series = 'male')
agesFit <- 0:100
yearsFit <- 1980:2020
#Fit the Lee Carter Model
LCFit <- fit(LC, data = AUSmale, ages.fit = agesFit,
years.fit = yearsFit)
AUSdata <- hmd.mx(country = 'AUS', username = 'maxelli1201@gmail.com',
password = 'Fisher1930$')
plot(AUSdata, series = 'male')
AUSmale <- StMoMoData(AUSdata, series = 'male')
agesFit <- 0:100
yearsFit <- 1980:2020
#Fit the Lee Carter Model
LCFit <- fit(LC, data = AUSmale, ages.fit = agesFit,
years.fit = yearsFit)
LC <- lc()
#Fit the Lee Carter Model
LCFit <- fit(LC, data = AUSmale, ages.fit = agesFit,
years.fit = yearsFit)
plot(LCFit)
LCforecast <- forecast(LCFit, h = 50)
plot(LCforecast)
plot(LCforecast, parametricbx = F)
#Simulating data with the model
LCSim <- simulate(LCFit, n = 1000, h = 50)
#Create simulation plot
plot(LCFit$years, LCFit$kt[1,], type = 'l', xlim = c(1980,2070),
ylim = c(-100,40), xlab = 'year', ylab = 'kt')
matlines(LCSim$kt.s$years,LCSim$kt.s$sim[1,,1:50], type = 'l',
lty = 1)
#Create simulation plot
plot(LCFit$years, LCFit$kt[1,], type = 'l', xlim = c(1980,2070),
ylim = c(-150,100), xlab = 'year', ylab = 'kt')
matlines(LCSim$kt.s$years,LCSim$kt.s$sim[1,,1:50], type = 'l',
lty = 1)
#Create simulation plot
plot(LCFit$years, LCFit$kt[1,], type = 'l', xlim = c(1980,2070),
ylim = c(-200,100), xlab = 'year', ylab = 'kt')
matlines(LCSim$kt.s$years,LCSim$kt.s$sim[1,,1:50], type = 'l',
lty = 1)
#Create simulation plot
plot(LCFit$years, LCFit$kt[1,], type = 'l', xlim = c(1980,2070),
ylim = c(-200,100), xlab = 'year', ylab = 'kt', main = 'Period Index')
matlines(LCSim$kt.s$years,LCSim$kt.s$sim[1,,1:50], type = 'l',
lty = 1)
#Residuals
res <- residuals(LCFit)
#Plot Residuals
plot(res, type = 'colormap', reslim = c(-10,10))
#Plot Residuals
plot(res, type = 'colourmap', reslim = c(-10,10))
#Plot Residuals
plot(res, type = 'colourmap', reslim = c(-5,5))
#Lee Carter Modeling
rm(list=ls())
library(StMoMo)
library(demography)
USAdata <- hmd.mx(country = 'USA', username = 'maxelli1201@gmail.com',
password = 'Fisher1930$')
plot(USAdata, series = 'male')
View(USAdata)
#Lee Carter Modeling
rm(list=ls())
library(StMoMo)
library(demography)
USAdata <- hmd.mx(country = 'USA', username = 'maxelli1201@gmail.com',
password = 'Fisher1930$')
plot(USAdata, series = 'total')
USA <- StMoMoData(USAdata, series = 'total')
View(USAdata)
USAdata[["rate"]][["total"]]
plot(USAdata[['rate']][['total']][,1], series = 'total')
plot(USAdata[['rate']][['total']], series = 'total')
plot(USAdata, series = 'total')
USA <- StMoMoData(USAdata, series = 'total')
agesFit <- 0:99
yearsFit <- 1980:2020
LC <- lc()
#Fit the Lee Carter Model
LCFit <- fit(LC, data = USA, ages.fit = agesFit,
years.fit = yearsFit)
plot(LCFit)
#Forecasting with the model
LCforecast <- forecast(LCFit, h = 50)
plot(LCforecast, parametricbx = F)
#Residuals
res <- residuals(LCFit)
#Plot Residuals
plot(res, type = 'colourmap', reslim = c(-5,5))
#Simulating data with the model
LCSim <- simulate(LCFit, n = 1000, h = 50)
#Create simulation plot
plot(LCFit$years, LCFit$kt[1,], type = 'l', xlim = c(1980,2070),
ylim = c(-200,100), xlab = 'year', ylab = 'kt', main = 'Period Index')
matlines(LCSim$kt.s$years,LCSim$kt.s$sim[1,,1:50], type = 'l',
lty = 1)
#Create simulation plot
plot(LCFit$years, LCFit$kt[1,], type = 'l', xlim = c(1980,2070),
ylim = c(-100,100), xlab = 'year', ylab = 'kt', main = 'Period Index')
matlines(LCSim$kt.s$years,LCSim$kt.s$sim[1,,1:50], type = 'l',
lty = 1)
library(tidyverse)
library(dplyr)
library(ggplot2)
#Read in all city data into a data frame
Asheville <- read.csv("RawData/AshevilleSeptember.csv")
setwd("C:/Users/maxel/Documents/GitHub/Max-Ellingsen/Project1-ECNS460")
library(tidyverse)
library(dplyr)
library(ggplot2)
#Read in all city data into a data frame
Asheville <- read.csv("RawData/AshevilleSeptember.csv")
setwd("C:/Users/maxel/Documents/GitHub/Max-Ellingsen/Project1-ECNS460/RawData")
library(tidyverse)
library(dplyr)
library(ggplot2)
#Read in all city data into a data frame
Asheville <- read.csv("RawData/AshevilleSeptember.csv")
setwd("C:/Users/maxel/Documents/GitHub/Max-Ellingsen")
library(tidyverse)
library(dplyr)
library(ggplot2)
#Read in all city data into a data frame
Asheville <- read.csv("RawData/AshevilleSeptember.csv")
setwd("C:/Users/maxel/Documents/GitHub/Max-Ellingsen/Project1-ECNS460")
library(tidyverse)
library(dplyr)
library(ggplot2)
#Read in all city data into a data frame
Asheville <- read.csv("RawData/AshevilleSeptember.csv")
